{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "# Tutorial 22: Transformer Model\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Architecture of a Transformer Model\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- Python, PyTorch, Deep Learning Training, Stochastic Gradient Descent\n",
    "\n",
    "My contact:\n",
    "\n",
    "- Niklas Beuter (niklas.beuter@th-luebeck.de)\n",
    "\n",
    "Course:\n",
    "\n",
    "- Slides and notebooks will be available at https://lernraum.th-luebeck.de/course/view.php?id=5383\n",
    "\n",
    "## Expected Outcomes\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Transformers\n",
    "\n",
    "Transformers are a type of neural network architecture that has revolutionized the field of natural language processing (NLP) and has been extended to other domains such as computer vision. Introduced by Vaswani et al. in the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) in 2017, transformers have become the backbone of many state-of-the-art models like BERT, GPT-3, and T5.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "At the core of transformers is the attention mechanism, which allows the model to focus on different parts of the input sequence when producing each output element. The attention mechanism computes a weighted sum of input elements, where the weights are determined dynamically based on the input.\n",
    "\n",
    "### Self-Attention\n",
    "\n",
    "Self-attention, also known as intra-attention, is a type of attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. This is crucial for capturing dependencies regardless of their distance in the input sequence.\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Transformers use multi-head attention to enhance the model's ability to focus on different parts of the sequence from multiple perspectives. This involves running several attention mechanisms in parallel, known as \"heads,\" and then concatenating their outputs.\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Since transformers do not have a built-in notion of the order of the sequence (unlike RNNs), positional encoding is used to inject information about the position of each token in the sequence. This is done by adding a set of sine and cosine functions of different frequencies to the input embeddings.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The encoder consists of multiple identical layers, each containing two main components:\n",
    "1. **Multi-Head Self-Attention Mechanism**: Allows the model to weigh the relevance of different tokens in the input sequence.\n",
    "2. **Position-wise Feed-Forward Neural Network**: Applied independently to each position, this consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "### Decoder\n",
    "\n",
    "The decoder also consists of multiple identical layers, with an additional component:\n",
    "1. **Masked Multi-Head Self-Attention**: Similar to the encoder's self-attention, but prevents attending to future tokens in the sequence to ensure the model is autoregressive.\n",
    "2. **Encoder-Decoder Attention**: Allows the decoder to focus on relevant parts of the input sequence.\n",
    "3. **Position-wise Feed-Forward Neural Network**: Same as in the encoder.\n",
    "\n",
    "### Training\n",
    "\n",
    "Transformers are typically trained using teacher forcing, where the input to the decoder at each time step is the ground truth token from the training dataset. This helps the model learn more effectively but requires careful handling during inference to prevent exposure bias.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Transformers have been successfully applied to a wide range of NLP tasks:\n",
    "- **Machine Translation**: Models like T5 and MarianMT provide state-of-the-art translation capabilities.\n",
    "- **Text Summarization**: BART and T5 can generate concise and coherent summaries of long documents.\n",
    "- **Question Answering**: BERT and its variants excel in understanding context and retrieving accurate answers from text.\n",
    "- **Text Generation**: GPT-3 has demonstrated impressive text generation capabilities, from writing essays to generating code.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Transformers have significantly advanced the capabilities of neural networks in NLP and beyond. Their ability to handle long-range dependencies and parallelize training has opened up new possibilities in various domains. As research continues, we can expect transformers to play an even more prominent role in machine learning applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision accelerate transformers torchtext spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Preparation of key, queries and values\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Multiplication of queries and keys (in batch)\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Softmax and Normalization by 1/sqrt(d) with d=embedding size\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        # Multiplication of result with values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Dropout, Norm and Skip connection (attention + query, forward + x)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(embed_size, heads, dropout=dropout, forward_expansion=forward_expansion)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(embed_size, heads)\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        # Dropout, Normalization and Skip connection\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=256, num_layers=6, forward_expansion=4, heads=8, dropout=0, device=\"cuda\", max_length=100):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
    "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length)\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1, trg_len, trg_len)\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n",
    "\n",
    "# Beispiel: Initialisierung und Durchführen eines Vorwärtslaufs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_vocab_size = 10000\n",
    "trg_vocab_size = 10000\n",
    "src_pad_idx = 1\n",
    "trg_pad_idx = 1\n",
    "\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n",
    "src = torch.randint(0, src_vocab_size, (32, 100)).to(device)\n",
    "trg = torch.randint(0, trg_vocab_size, (32, 100)).to(device)\n",
    "\n",
    "out = model(src, trg)\n",
    "print(out.shape)  # Erwartet: [32, 100, trg_vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of a small training for a translation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Laden der SpaCy Modelle, um den jeweiligen Tokenizer zu verwenden\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenizer Funktionen für deutsch und englisch\n",
    "def tokenize_de(text):\n",
    "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# Daten vorverarbeiten und Vokabular erstellen\n",
    "def build_vocab(tokenized_texts, min_freq=1):\n",
    "    freq = {}\n",
    "    for text in tokenized_texts:\n",
    "        for token in text:\n",
    "            if token in freq:\n",
    "                freq[token] += 1\n",
    "            else:\n",
    "                freq[token] = 1\n",
    "    \n",
    "    vocab = {token: idx for idx, (token, count) in enumerate(freq.items()) if count >= min_freq}\n",
    "    vocab['<pad>'] = len(vocab)\n",
    "    vocab['<sos>'] = len(vocab)\n",
    "    vocab['<eos>'] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Datensatz und DataLoader erstellen\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, trg_texts, src_vocab, trg_vocab):\n",
    "        self.src_texts = src_texts\n",
    "        self.trg_texts = trg_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_texts[idx]\n",
    "        trg = self.trg_texts[idx]\n",
    "        \n",
    "        src_indices = [self.src_vocab.get('<sos>')] + [self.src_vocab.get(token, self.src_vocab['<pad>']) for token in src] + [self.src_vocab.get('<eos>')]\n",
    "        trg_indices = [self.trg_vocab.get('<sos>')] + [self.trg_vocab.get(token, self.trg_vocab['<pad>']) for token in trg] + [self.trg_vocab.get('<eos>')]\n",
    "        \n",
    "        return torch.tensor(src_indices), torch.tensor(trg_indices)\n",
    "\n",
    "# Padding-Funktion\n",
    "def pad_sequences(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    \n",
    "    src_lens = [len(seq) for seq in src_batch]\n",
    "    trg_lens = [len(seq) for seq in trg_batch]\n",
    "    \n",
    "    max_src_len = max(src_lens)\n",
    "    max_trg_len = max(trg_lens)\n",
    "    \n",
    "    padded_src = torch.zeros(len(src_batch), max_src_len).long()\n",
    "    padded_trg = torch.zeros(len(trg_batch), max_trg_len).long()\n",
    "    \n",
    "    for i, (src_len, trg_len) in enumerate(zip(src_lens, trg_lens)):\n",
    "        padded_src[i, :src_len] = src_batch[i]\n",
    "        padded_trg[i, :trg_len] = trg_batch[i]\n",
    "    \n",
    "    return padded_src, padded_trg\n",
    "\n",
    "# Beispiel-Datensätze laden (hier nur für Demonstrationszwecke drei Sätze)\n",
    "src_texts = [\"ein beispiel satz\", \"noch ein beispiel\", \"ein letzter satz\"]\n",
    "trg_texts = [\"a sample sentence\", \"another example\", \"one last sentence\"]\n",
    "\n",
    "src_tokenized = [tokenize_de(text) for text in src_texts]\n",
    "trg_tokenized = [tokenize_en(text) for text in trg_texts]\n",
    "\n",
    "src_vocab = build_vocab(src_tokenized)\n",
    "trg_vocab = build_vocab(trg_tokenized)\n",
    "\n",
    "# Sicherstellen, dass alle Tokens im Vokabular sind\n",
    "for text in src_tokenized:\n",
    "    for token in text:\n",
    "        if token not in src_vocab:\n",
    "            src_vocab[token] = len(src_vocab)\n",
    "for text in trg_tokenized:\n",
    "    for token in text:\n",
    "        if token not in trg_vocab:\n",
    "            trg_vocab[token] = len(trg_vocab)\n",
    "\n",
    "# Datensätze und DataLoader erstellen\n",
    "dataset = TranslationDataset(src_tokenized, trg_tokenized, src_vocab, trg_vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=pad_sequences)\n",
    "\n",
    "# Modell laden\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src_pad_idx = src_vocab['<pad>']\n",
    "trg_pad_idx = trg_vocab['<pad>']\n",
    "trg_sos_idx = trg_vocab['<sos>']\n",
    "trg_eos_idx = trg_vocab['<eos>']\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "trg_vocab_size = len(trg_vocab)\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx).to(device)\n",
    "\n",
    "# Verlustfunktion und Optimierer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Trainingsschleife\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (src, trg) in enumerate(data_loader):\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        # Dies ist der entscheidende Schritt für das Training\n",
    "        # Beim Input wird das letzte Wort entfernt, welches dann beim Target vorhergesagt werden soll\n",
    "        trg_input = trg[:, :-1]\n",
    "        trg_target = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg_input)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "\n",
    "        loss = criterion(output, trg_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training abgeschlossen.\")\n",
    "\n",
    "# Methode zum Generieren von Vorhersagen\n",
    "def translate_sentence(model, sentence, src_vocab, trg_vocab, device, max_length=50):\n",
    "    model.eval()\n",
    "    tokens = tokenize_de(sentence)\n",
    "    # Hinzufügen von start und end-token\n",
    "    tokens = [src_vocab['<sos>']] + [src_vocab[token] for token in tokens] + [src_vocab['<eos>']]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_vocab['<sos>']]\n",
    "\n",
    "    for i in range(max_length):\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        with torch.no_grad():\n",
    "            output = model.decoder(trg_tensor, enc_src, src_mask, trg_mask)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [list(trg_vocab.keys())[list(trg_vocab.values()).index(i)] for i in trg_indexes]\n",
    "    return trg_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel für das Testen des Modells\n",
    "example_sentence = \"ein beispiel satz\"\n",
    "prediction = translate_sentence(model, example_sentence, src_vocab, trg_vocab, device)\n",
    "\n",
    "print(f\"Input Sentence: {example_sentence}\")\n",
    "print(f\"Predicted Translation: {' '.join(prediction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_vocab.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
