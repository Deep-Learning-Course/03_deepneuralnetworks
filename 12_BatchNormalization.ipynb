{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "# Tutorial 12: Batch Normalization\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Batch Normalization, Normalization Layers\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- Python, Tensor basics, DNN Training\n",
    "\n",
    "My contact:\n",
    "\n",
    "- Niklas Beuter (niklas.beuter@th-luebeck.de)\n",
    "\n",
    "Course:\n",
    "\n",
    "- Slides and notebooks will be available at https://lernraum.th-luebeck.de/course/view.php?id=5383\n",
    "\n",
    "## Expected Outcomes\n",
    "* Understand why, where and when we could use Batch normalization or other normalization layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following example demonstrates the problem of a covariance shift\n",
    "\n",
    "In this code, the training data is generated with a mean of 0 and standard deviation of 1, while the test data has a mean of 2 and standard deviation of 1.5. This difference in distributions between the training and test sets is a classic example of covariate shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data(mean, std, num_samples=1000):\n",
    "    \"\"\" Generate data with a normal distribution based on the mean and std. \"\"\"\n",
    "    return np.random.normal(mean, std, num_samples)\n",
    "\n",
    "def plot_data(train_data, test_data):\n",
    "    \"\"\" Plot histograms of the training and test data. \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(train_data, bins=30, alpha=0.5, label='Training Data')\n",
    "    plt.hist(test_data, bins=30, alpha=0.5, label='Test Data')\n",
    "    plt.title('Distribution of Training and Test Data')\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Parameters for generating data\n",
    "mean_train, std_train = 0, 1\n",
    "mean_test, std_test = 2, 1.5  # Different mean and std for test data to introduce covariate shift\n",
    "\n",
    "# Generate training and test data\n",
    "train_data = generate_data(mean_train, std_train)\n",
    "test_data = generate_data(mean_test, std_test)\n",
    "\n",
    "# Plot the data to show covariate shift\n",
    "plot_data(train_data, test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to normalize the data that the distribution shift is removed. This is also valid for different batches of data. We want to remove any shifts there as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an own batch norm function, which uses the batch *X*, the parameters of the batch norm $\\gamma, \\beta$, the *moving_mean* and *moving_var*, which are used during test time. The parameter *eps* is used for numerical stability and to avoid a division by 0. The momentum can be optionally used also in batch norm to get a better moving mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps=1e-5, momentum=0.1):\n",
    "    if not torch.is_grad_enabled():\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        if len(X.shape) == 2:\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
    "    \n",
    "    return gamma * X_hat + beta, moving_mean, moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        shape = (1, num_features, 1, 1) if num_dims == 4 else (1, num_features)\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean, self.moving_var)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the LeNet architecture and add the batch norm to it. We add the batchnorm before the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNLeNetScratch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # Layer 1: Convolutional + BatchNorm + Activation + Pooling\n",
    "            # Input 1 channel (we input an image with only gray channel), but 6 channel output\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            BatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 2: Convolutional + BatchNorm + Activation + Pooling\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            BatchNorm(16, num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            # Flatten the output for the fully connected layer\n",
    "            nn.Flatten(),\n",
    "            # Layer 3: Fully Connected + BatchNorm + Activation\n",
    "            # Here, we \n",
    "            nn.Linear(256, 120),  # Adjust the size according to the output from the last pooling layer\n",
    "            BatchNorm(120, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            # Layer 4: Fully Connected + BatchNorm + Activation\n",
    "            nn.Linear(120, 84),\n",
    "            BatchNorm(84, num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            # Output layer with 10 classes\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.to(device)\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transform compose is used to combine several transformations. In this case, it converts the images and normalizes them\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "# Load datasets MNIST\n",
    "#train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "#test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "# Load datasets FASHIONMNIST\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, Loss, and Optimizer\n",
    "model = BNLeNetScratch()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, test_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## learned gamma and beta parameters\n",
    "def print_bn_parameters(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, BatchNorm):  # Check if it is the custom BatchNorm\n",
    "            gamma = module.gamma.data  # Access the gamma parameter\n",
    "            beta = module.beta.data  # Access the beta parameter\n",
    "            print(f\"{name} - Gamma: {gamma}, Beta: {beta}\")\n",
    "\n",
    "# Assuming model is your trained model instance\n",
    "print_bn_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation from PyTorch (using BatchNorm2d for convolution layer and BatchNorm1D for fully connected layer)\n",
    "class BNLeNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BNLeNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            # Layer 1: Convolutional + BatchNorm + Activation + Pooling\n",
    "            # Input 1 channel (we input an image with only gray channel), but 6 channel output\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Layer 2: Convolutional + BatchNorm + Activation + Pooling\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Flatten the output for the fully connected layer\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Layer 3: Fully Connected + BatchNorm + Activation\n",
    "            nn.Linear(256, 120),  # Adjust the size according to the output from the last pooling layer\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.Sigmoid(),\n",
    "\n",
    "            # Layer 4: Fully Connected + BatchNorm + Activation\n",
    "            nn.Linear(120, 84),\n",
    "            nn.BatchNorm1d(84),\n",
    "            nn.Sigmoid(),\n",
    "\n",
    "            # Output layer\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(model, train_loader, test_loader, criterion, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
