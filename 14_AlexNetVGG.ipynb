{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "# Tutorial 14: AlexNet and VGG architectures\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Architectures for Deep Neural Networks AlexNet 2012, VGG 2014\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- Python, Tensor basics, PyTorch\n",
    "\n",
    "My contact:\n",
    "\n",
    "- Niklas Beuter (niklas.beuter@th-luebeck.de)\n",
    "\n",
    "Course:\n",
    "\n",
    "- Slides and notebooks will be available at https://lernraum.th-luebeck.de/course/view.php?id=5383\n",
    "\n",
    "## Expected Outcomes\n",
    "* Understand the basic components of neural networks: layers, neurons, weights, biases, activations, and loss functions.\n",
    "* Gain hands-on experience with the computational aspects of setting up neural networks, including training and usage.\n",
    "* Learn how to add layers with correct sizes to a deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "\n",
    "[AlexNet](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) is a pioneering convolutional neural network (CNN) that significantly influenced the field of deep learning, especially in computer vision. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet was the winning entry in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012.\n",
    "\n",
    "## Key Features of AlexNet\n",
    "\n",
    "- **Architecture**:\n",
    "  - AlexNet contains eight layers; the first five are convolutional layers, and the last three are fully connected layers. It uses ReLU (Rectified Linear Unit) for the non-linear part, instead of a sigmoid or tanh function, which helps to alleviate the vanishing gradient problem.\n",
    "  - The network uses overlapping pooling, which helps to reduce the size of the network and provides better performance than the traditional non-overlapping scheme.\n",
    "  - AlexNet used an early version of Normalization (Local Response Normalization). You can find a description and comparison to BatchNorm further down.\n",
    "\n",
    "- **Training**:\n",
    "  - The network was trained on two GTX 580 GPUs for about five to six days.\n",
    "  - It uses data augmentation techniques such as random crops, flips, and color alterations to expand the training dataset and improve generalization.\n",
    "  - Dropout layers are applied before the first and second fully connected layers to prevent overfitting.\n",
    "\n",
    "- **Impact**:\n",
    "  - AlexNet significantly outperformed the second-place competitor in the ILSVRC-2012 competition by reducing the top-5 error from 25.8% to 16.4%.\n",
    "  - Its success demonstrated the potential of deep neural networks, particularly CNNs, leading to widespread adoption in the field of image recognition and beyond.\n",
    "\n",
    "## Local Response Normalization (LRN) vs Batch Normalization (BatchNorm)\n",
    "\n",
    "### Local Response Normalization (LRN)\n",
    "\n",
    "**Concept**: LRN is inspired by a biological process known as lateral inhibition, a neurobiology concept where activated neurons inhibit the activity of neighboring neurons in the same layer, creating competitive dynamics among the neurons of a layer.\n",
    "\n",
    "**How LRN Works**:\n",
    "- LRN applies a normalization over local input regions in convolutional neural networks. For a given pixel in a feature map, normalization is applied across the channels.\n",
    "- The formula for LRN is:\n",
    "\n",
    "  $$b_{x,y}^i = \\frac{a_{x,y}^i}{\\left( k + \\alpha \\sum_{j=\\max(0, i-n/2)}^{\\min(N-1, i+n/2)} (a_{x,y}^j)^2 \\right)^\\beta}$$\n",
    "  \n",
    "  Where:\n",
    "  - $b_{x,y}^i$ is the normalized output.\n",
    "  - $a_{x,y}^i$ is the activity of a neuron computed by applying kernel $i$ at position $(x, y)$ and then applying the ReLU nonlinearity.\n",
    "  - $n$ is the size of the local neighborhood to normalize over (across channels).\n",
    "  - $k, \\alpha, \\beta$ are hyperparameters.\n",
    "  - $N$ is the total number of channels in the layer.\n",
    "\n",
    "### Batch Normalization (BatchNorm)\n",
    "\n",
    "**Concept**: Batch Normalization normalizes the activations of a neuron across the mini-batch, addressing internal covariate shift by standardizing the inputs to a layer for each mini-batch.\n",
    "\n",
    "**How BatchNorm Works**:\n",
    "- BatchNorm normalizes the input for each neuron to zero mean and unit variance, then scales and shifts it using two learnable parameters per neuron. The formula is:\n",
    "  \n",
    "  $$\\hat{x}^{(k)} = \\frac{x^{(k)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "\n",
    "  Followed by:\n",
    "\n",
    "  $$y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}$$\n",
    "\n",
    "  Where:\n",
    "  - $x^{(k)}$ are the inputs to the layer.\n",
    "  - $\\mu_B$ and $\\sigma_B^2$ are the mean and variance computed over the batch.\n",
    "  - $\\gamma$ and $\\beta$ are learnable parameters.\n",
    "  - $\\epsilon$ is a small constant added for numerical stability.\n",
    "\n",
    "## Key Differences\n",
    "- **Scope**: LRN works across channels enhancing contrast locally, whereas BatchNorm normalizes activations across the batch for each neuron.\n",
    "- **Impact**: BatchNorm reduces the amount by which the hidden unit values shift around (covariate shift), while LRN's impact is more on local neuron competition.\n",
    "- **Performance and Usage**: BatchNorm tends to outperform LRN and has largely replaced LRN in modern network architectures due to its effectiveness in accelerating training and reducing the sensitivity to network initialization.\n",
    "\n",
    "\n",
    "## Challenges and Limitations\n",
    "\n",
    "- Requires significant computational resources, notably GPUs, for training and inference.\n",
    "- Deep architecture necessitates a large number of parameters, making it prone to overfitting on smaller datasets without proper regularization.\n",
    "\n",
    "## Legacy\n",
    "\n",
    "AlexNet has been a foundational model that spurred further research and developments in deep learning. It opened the door to more complex architectures like VGG, ResNet, and many others that have since driven progress in computer vision tasks.\n",
    "\n",
    "\n",
    "## Step 1: Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the network\n",
    "\n",
    "Remember the calculation of the image size using the following formula:\n",
    "\n",
    "$$ W_{new} = \\frac{W_{old}-K+P*2}{S}+1 $$\n",
    "\n",
    "with $K$ Kernel Size (Filter Size), $P$ Padding Size (image border), $S$ Stride (shift of Kernel over the image).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # First convolutional layer\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        \n",
    "        # Third convolutional layer\n",
    "        \n",
    "        # Fourth convolutional layer\n",
    "        \n",
    "        # Fifth convolutional layer\n",
    "        \n",
    "        # Max pooling layer\n",
    "        \n",
    "        # Fully connected layers\n",
    "\n",
    "        # Dropout layer\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First block (conv1, relu, pooling)\n",
    "        \n",
    "        # Second block (conv2, relu, pooling)\n",
    "        \n",
    "        # Third block (conv3, relu)\n",
    "        \n",
    "        # Fourth block (conv4, relu)\n",
    "        \n",
    "        # Fifth block (conv5, relu, pooling)\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)  # Adjust the flattening size based on your input size\n",
    "        # Fully connected layers fc1, fc2 with relu, then dropout\n",
    "\n",
    "        # Output layer (fc3)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up data loaders\n",
    "\n",
    "Here, we use the [Cifar10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) with 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),  # Resizing the images to fit AlexNet input dimensions\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denormalize(image, means=[0.485, 0.456, 0.406], stds=[0.229, 0.224, 0.225]):\n",
    "    \"\"\" Reverses normalization on an image given the mean and std \"\"\"\n",
    "    means = torch.tensor(means).view(3, 1, 1)\n",
    "    stds = torch.tensor(stds).view(3, 1, 1)\n",
    "    return image * stds + means\n",
    "\n",
    "def visualize_one_image_per_class(dataset):\n",
    "    num_classes = 10  # There are 10 classes in CIFAR-10\n",
    "    class_images = {}\n",
    "    labels = range(num_classes)\n",
    "\n",
    "    # Loop over the dataset to find one image per class\n",
    "    for image, label in dataset:\n",
    "        if label in labels and label not in class_images:\n",
    "            class_images[label] = image\n",
    "        if len(class_images) == num_classes:  # Stop once we have one image per class\n",
    "            break\n",
    "\n",
    "    # Plotting the images\n",
    "    fig, axes = plt.subplots(1, num_classes, figsize=(15, 15))\n",
    "    for label, image in class_images.items():\n",
    "        ax = axes[label]\n",
    "        # Denormalize and permute image to fit matplotlib expectation\n",
    "        image = denormalize(image)  # Denormalize the image\n",
    "        image = image.permute(1, 2, 0)  # Permute the tensor to (H, W, C)\n",
    "        image = image.clip(0, 1)  # Ensure the image values are between 0 and 1\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Class {label}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize one image per class from the dataset\n",
    "visualize_one_image_per_class(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Network and Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = AlexNet()\n",
    "model.to(device)  # Move the model to the appropriate device\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the created model\n",
    "print('The model:')\n",
    "print(model)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (simplified)\n",
    "def train(model, device, train_loader, criterion, optimizer, epochs):\n",
    "    model.train() # set model in training mode\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate the Network\n",
    "\n",
    "Using `model.eval()` correctly is crucial for getting accurate evaluation or test results because it ensures the model behaves predictably and does not use mechanisms like dropout, which are meant to introduce randomness during training only. This helps in making fair comparisons between the model's performance during training and testing phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run the Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, device, train_loader, criterion, optimizer, epochs=10)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Predict classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_visualize_single_image(model, image, label):\n",
    "    # Define the transform to resize the image and normalize it as expected by the model\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227, 227)),  # Resize to the input size expected by the model\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image = image.to(device)  # Move image to the device\n",
    "    image = transform(image)  # Apply the transformation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = image.unsqueeze(0)  # Add batch dimension\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "    # Move the image back to CPU for visualization and undo normalization\n",
    "    image = image.cpu().squeeze(0)  # Remove batch dimension\n",
    "    image = image.permute(1, 2, 0)  # Change from CxHxW to HxWxC for visualization\n",
    "    image = image * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])  # Undo normalization\n",
    "    image = image.clip(0, 1)  # Ensure the image values are between 0 and 1\n",
    "\n",
    "    # Plot the image and its predicted class\n",
    "    plt.imshow(image)\n",
    "    plt.title(f'Actual: {label.item()} Predicted: {predicted.item()}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one batch from the test_loader\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "\n",
    "print(\"Number of images in batch: \", test_images.shape[0])\n",
    "\n",
    "# Select one image and label from the batch\n",
    "image, label = test_images[50], test_labels[50]\n",
    "\n",
    "# Predict the class \n",
    "predict_and_visualize_single_image(model, image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the weights of the first layer as images\n",
    "\n",
    "In order to interprete the features of the first layer, we can plot the weights as images. Then, we see where the weights are focussing on. Usually, the convolution layer learn features or feature combinations to describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_weights(model):\n",
    "    # Assuming the model's first convolutional layer weights are of the shape [out_channels, in_channels, height, width]\n",
    "    weights = model.conv1.weight.data.cpu().numpy()\n",
    "\n",
    "    # Determine the number of filters in the first convolutional layer\n",
    "    num_filters = weights.shape[0]\n",
    "    \n",
    "    # Number of columns for subplot\n",
    "    num_cols = min(10, num_filters)\n",
    "    num_rows = (num_filters + num_cols - 1) // num_cols  # Ceiling division\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 1.5, num_rows * 1.5))\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i < num_filters:\n",
    "            # Assuming the input channel is RGB (3 channels), you can average over the channels, or just take one channel\n",
    "            img = weights[i][0]  # Taking the first channel (for RGB images)\n",
    "            # Normalize weight for better visualization\n",
    "            img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "            ax.imshow(img, cmap='gray')\n",
    "            ax.set_title(f'Filter {i+1}')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# plot weights\n",
    "plot_weights(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG\n",
    "\n",
    "[VGG (Visual Geometry Group)](https://arxiv.org/abs/1409.1556v6) is a convolutional neural network architecture proposed by the Visual Geometry Group at the University of Oxford.\n",
    "\n",
    "## Key characteristics\n",
    "\n",
    "1. Depth: It consists of very deep networks (up to 19 layers) with small (3x3) convolution filters.\n",
    "2. Configurations: VGG networks are defined by their architecture configurations, denoted by VGG followed by a number (e.g., VGG16, VGG19), indicating the number of weight layers.\n",
    "3. Simple Structure: VGG networks have a simple and uniform architecture, making them easy to understand and implement.\n",
    "4. Transfer Learning: Pre-trained VGG models on large-scale image datasets (e.g., ImageNet) are commonly used for transfer learning tasks due to their effectiveness in feature extraction.\n",
    "\n",
    "## Basic building blocks\n",
    "\n",
    "1. Convolutional Layers: Consist of multiple convolutional layers with 3x3 filters, followed by ReLU activation functions.\n",
    "2. Max Pooling Layers: Spatial pooling layers with 2x2 filters and stride 2, used to reduce spatial dimensions.\n",
    "3. Fully Connected Layers: One or more fully connected layers with ReLU activation functions, followed by a softmax layer for classification.\n",
    "\n",
    "The VGG network achieves strong performance on image classification tasks, and its simplicity and effectiveness have made it a popular choice in deep learning research and applications.\n",
    "\n",
    "## Step 1: Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_ForCIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGG16_ForCIFAR10, self).__init__()\n",
    "        self.width = \n",
    "        self.height = \n",
    "        self.features = nn.Sequential(\n",
    "            # Conv Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),  # Changed from 1 to 3 input channels\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Conv Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Conv Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Conv Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Conv Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # The output of the last pooling layer would be 512x1x1 if the input is 32x32\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * self.height * self.width, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the feature maps into a vector\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2-3:\n",
    "\n",
    "Do them with the code above, if not already executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = VGG16_ForCIFAR10(num_classes=10)\n",
    "model.to(device)  # where your_device could be 'cuda' or 'cpu'\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5-7\n",
    "\n",
    "Do them with the code above, if not already executed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train the network\n",
    "\n",
    "Training time is quite intensive. It might took around 40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, device, train_loader, criterion, optimizer, epochs=10)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model tends to overfit on the data, but a test accuracy between Accuracy: 80-90% should be achieved. I got 83.49% with a first run without fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
