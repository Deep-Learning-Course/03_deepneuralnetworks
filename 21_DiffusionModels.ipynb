{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "# Tutorial 21: Diffusion Models for Image Generation\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Creating images with a Diffusion Model\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- Python, PyTorch, Deep Learning Training, Stochastic Gradient Descent\n",
    "\n",
    "My contact:\n",
    "\n",
    "- Niklas Beuter (niklas.beuter@th-luebeck.de)\n",
    "\n",
    "Course:\n",
    "\n",
    "- Slides and notebooks will be available at https://lernraum.th-luebeck.de/course/view.php?id=5383\n",
    "\n",
    "## Expected Outcomes\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Diffusion and Stable Diffusion Models\n",
    "\n",
    "## Diffusion Models\n",
    "\n",
    "Diffusion models are a class of generative models that aim to generate new data samples by iteratively refining noise into a desired output. The core idea behind diffusion models is to simulate the process of gradually adding noise to data and then learning how to reverse this process to recover the original data. This is achieved through a series of denoising steps.\n",
    "\n",
    "### Key Concepts in Diffusion Models\n",
    "\n",
    "1. **Forward Diffusion Process**: This process involves gradually adding Gaussian noise to the data over a sequence of time steps. By the end of this process, the data is transformed into pure noise.\n",
    "\n",
    "2. **Reverse Diffusion Process**: The reverse process aims to reconstruct the original data from the noisy data by iteratively removing the noise. A neural network is trained to predict the noise added at each step, which is then subtracted to recover the data.\n",
    "\n",
    "3. **Variational Inference**: Diffusion models often use variational inference techniques to learn the parameters of the noise and data distribution. This involves maximizing a variational lower bound on the likelihood of the data.\n",
    "\n",
    "There are three main components in latent diffusion -\n",
    "\n",
    "1. A text encoder, in this case, a [CLIP Text encoder](https://openai.com/blog/clip/)\n",
    "2. An autoencoder, in this case, a Variational Auto Encoder also referred to as VAE\n",
    "3. A [U-Net](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "## Stable Diffusion Models\n",
    "\n",
    "Stable diffusion models are an advanced class of diffusion models that enhance stability and efficiency in the diffusion process. They are particularly useful in generating high-quality images and have been popularized by models such as DALL-E 2 and Stable Diffusion.\n",
    "\n",
    "### Characteristics of Stable Diffusion Models\n",
    "\n",
    "Stable diffusion or latent diffusion has first been proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).The original Diffusion model tends to consume a lot more memory, so latent diffusion models were created which can do the diffusion process in lower dimension space called Latent Space\n",
    "\n",
    "1. **Latent Space Diffusion**: Instead of operating directly in the pixel space, stable diffusion models often perform diffusion in a lower-dimensional latent space. This reduces the computational complexity and improves the quality of the generated images.\n",
    "\n",
    "2. **Conditional Generation**: Stable diffusion models can be conditioned on various types of information, such as text descriptions, class labels, or other modalities. This allows for more controllable and diverse generation.\n",
    "\n",
    "3. **Score-Based Methods**: These models leverage score-based generative modeling techniques, where a score function (the gradient of the data distribution) is learned. This function guides the reverse diffusion process more effectively.\n",
    "\n",
    "### Applications of Stable Diffusion Models\n",
    "\n",
    "1. **Image Synthesis**: Generating high-quality images from textual descriptions or other forms of input data.\n",
    "\n",
    "2. **Inpainting**: Filling in missing parts of an image in a plausible manner.\n",
    "\n",
    "3. **Super-Resolution**: Enhancing the resolution of low-quality images.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Diffusion and stable diffusion models represent a powerful approach in generative modeling, with the ability to produce high-fidelity data samples. By understanding and leveraging the principles of the diffusion process, these models have opened new possibilities in fields such as computer vision, art generation, and beyond.\n",
    "\n",
    "---\n",
    "\n",
    "This introduction provides a foundational understanding of diffusion and stable diffusion models, highlighting their key concepts, characteristics, and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Text Encoder\n",
    "\n",
    "CLIP(Contrastive Language–Image Pre-training) text encoder takes the text as an input and generates text embeddings that are close in latent space as it may be if you would have encoded an image through a CLIP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch diffusers transformers accelerate torchvision fastdownload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, logging\n",
    "\n",
    "## disable warnings\n",
    "logging.disable(logging.WARNING)  \n",
    "\n",
    "## Import the CLIP artifacts \n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "## Initiating tokenizer and encoder.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize a prompt and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a dog wearing hat\"]\n",
    "tok =tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\") \n",
    "print(tok.input_ids.shape)\n",
    "tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer returns two objects in the form of a dictionary -\n",
    "1. input_ids - A tensor of size 1x77 as one prompt was passed and padded to 77 max length. 49406 is a start token, 320 is a token given to the word “a”, 1929 to the word \"dog\", 3309 to the word \"wearing\", 3801 to the word \"hat\", and 49407 is the end of text token repeated till the pad length of 77.\n",
    "2. attention_mask - 1 representing an embedded value and 0 representing padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in list(tok.input_ids[0,:7]): print(f\"{token}:{tokenizer.convert_ids_to_tokens(int(token))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let’s look at the Token_To_Embedding Encoder which takes the input_ids generated by the tokenizer and converts them into embeddings -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into embedding and reduce to half precision (16 Bit)\n",
    "emb = text_encoder(tok.input_ids.to(\"cuda\"))[0].half()\n",
    "print(f\"Shape of embedding : {emb.shape}\")\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, each tokenized input of size 1x77 has now been translated to 1x77x768 shape embedding. So, each word got represented in a 768-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE\n",
    "\n",
    "An autoencoder contains two parts -\n",
    "1. Encoder takes an image as input and converts it into a low dimensional latent representation\n",
    "2. Decoder takes the latent representation and converts it back into an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us downdload an example image\n",
    "\n",
    "## To import an image from a URL \n",
    "from fastdownload import FastDownload  \n",
    "## Imaging  library \n",
    "from PIL import Image \n",
    "from torchvision import transforms as tfms  \n",
    "## Basic libraries \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline  \n",
    "## Loading a VAE model \n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "def load_image(p):\n",
    "    '''     \n",
    "    Function to load images from a defined path     \n",
    "    '''\n",
    "    return Image.open(p).convert('RGB').resize((512,512))\n",
    "def pil_to_latents(image):\n",
    "    '''     \n",
    "    Function to convert image to latents     \n",
    "    '''     \n",
    "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0   \n",
    "    init_image = init_image.to(device=\"cuda\", dtype=torch.float16)\n",
    "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215     \n",
    "    return init_latent_dist  \n",
    "def latents_to_pil(latents):     \n",
    "    '''     \n",
    "    Function to convert latents to images     \n",
    "    '''     \n",
    "    latents = (1 / 0.18215) * latents     \n",
    "    with torch.no_grad():         \n",
    "        image = vae.decode(latents).sample     \n",
    "    \n",
    "    image = (image / 2 + 0.5).clamp(0, 1)     \n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()      \n",
    "    images = (image * 255).round().astype(\"uint8\")     \n",
    "    pil_images = [Image.fromarray(image) for image in images]        \n",
    "    return pil_images\n",
    "\n",
    "p = FastDownload().download('https://lafeber.com/pet-birds/wp-content/uploads/2018/06/Scarlet-Macaw-2.jpg')\n",
    "img = load_image(p)\n",
    "print(f\"Dimension of this image: {np.array(img).shape}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s compress this image by using the VAE encoder, we will be using the pil_to_latents helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_img = pil_to_latents(img)\n",
    "print(f\"Dimension of this latent representation: {latent_img.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see how the VAE compressed a 3 x 512 x 512 dimension image into a 4 x 64 x 64 image. That’s a compression ratio of 48x! Let’s visualize these four channels of latent representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for c in range(4):\n",
    "    axs[c].imshow(latent_img[0][c].detach().cpu(), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This latent representation in theory should capture a lot of information about the original image. Let’s use the decoder on this representation to see what we get back. For this, we will use the latents_to_pil helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_img = latents_to_pil(latent_img)\n",
    "decoded_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops\n",
    "\n",
    "# Berechne die Differenz\n",
    "difference = ImageChops.difference(img, decoded_img[0])\n",
    "\n",
    "# Konvertiere die Differenz in ein numpy-Array\n",
    "diff_array = np.array(difference)\n",
    "\n",
    "# Normalisiere die Differenz, um den Bereich auf [0, 255] zu skalieren\n",
    "normalized_diff = (diff_array - diff_array.min()) * (255.0 / (diff_array.max() - diff_array.min()))\n",
    "normalized_diff = normalized_diff.astype(np.uint8)\n",
    "\n",
    "# Konvertiere das normalisierte Array zurück in ein Bild\n",
    "normalized_diff_image = Image.fromarray(normalized_diff)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Bild 1\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(decoded_img[0])\n",
    "axes[1].set_title(\"Bild 2\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(normalized_diff_image)\n",
    "axes[2].set_title(\"Differenz\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the figure above VAE decoder was able to recover the original image from a 48x compressed latent representation. That’s impressive!\n",
    "\n",
    "Look at the difference. It is really low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net\n",
    "\n",
    "The U-Net model takes two inputs -\n",
    "1. Noisy latent or Noise- Noisy latents are latents produced by a VAE encoder (in case an initial image is provided) with added noise or it can take pure noise input in case we want to create a random new image based solely on a textual description\n",
    "2. Text embeddings - CLIP-based embedding generated by input textual prompts\n",
    "\n",
    "The output of the U-Net model is the predicted noise residual which the input noisy latent contains. In other words, it predicts the noise which is subtracted from the noisy latents to return the original de-noised latents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel, LMSDiscreteScheduler\n",
    "## Initializing a scheduler\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "## Setting number of sampling steps\n",
    "scheduler.set_timesteps(51)\n",
    "## Initializing the U-Net model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed from code above, we not only imported unet but also a scheduler. The purpose of a schedular is to determine how much noise to add to the latent at a given step in the diffusion process. Let’s visualize the schedular function\n",
    "\n",
    "The diffusion process follows this sampling schedule where we start with high noise and gradually denoise the image. Let’s visualize this process -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn_like(latent_img) # Random noise\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 12))\n",
    "for c, sampling_step in enumerate(range(0,51,10)):\n",
    "    encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[sampling_step]]))\n",
    "    axs[c//3][c%3].imshow(latents_to_pil(encoded_and_noised)[0])\n",
    "    axs[c//3][c%3].set_title(f\"Step - {sampling_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how a U-Net removes the noise from the image. Let’s start by adding some noise to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_and_noised = scheduler.add_noise(latent_img, noise, timesteps=torch.tensor([scheduler.timesteps[40]]))\n",
    "latents_to_pil(encoded_and_noised)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s run through U-Net and try to de-noise this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unconditional textual prompt\n",
    "prompt = [\"\"]\n",
    "## Using clip model to get embeddings\n",
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "with torch.no_grad(): \n",
    "    text_embeddings = text_encoder(\n",
    "        text_input.input_ids.to(\"cuda\")\n",
    "    )[0]\n",
    "    \n",
    "## Using U-Net to predict noise    \n",
    "latent_model_input = torch.cat([encoded_and_noised.to(\"cuda\").float()]).half()\n",
    "with torch.no_grad():\n",
    "    noise_pred = unet(\n",
    "        latent_model_input,40,encoder_hidden_states=text_embeddings\n",
    "    )[\"sample\"]\n",
    "## Visualize after subtracting noise \n",
    "latents_to_pil(encoded_and_noised- noise_pred)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the U-Net output is clearer than the original noisy input passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What’s their role in the Stable diffusion pipeline\n",
    "Latent diffusion uses the U-Net to gradually subtract noise in the latent space over several steps to reach the desired output. With each step, the amount of noise added to the latents is reduced till we reach the final de-noised output. U-Nets were first introduced by this paper for Biomedical image segmentation. The U-Net has an encoder and a decoder which are comprised of ResNet blocks. The stable diffusion U-Net also has cross-attention layers to provide them with the ability to condition the output based on the text description provided. The Cross-attention layers are added to both the encoder and the decoder part of the U-Net usually between ResNet blocks. You can learn more about this U-Net architecture here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, logging\n",
    "## disable warnings\n",
    "logging.disable(logging.WARNING)  \n",
    "## Imaging  library\n",
    "from PIL import Image\n",
    "from torchvision import transforms as tfms\n",
    "## Basic libraries\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "import shutil\n",
    "import os\n",
    "## For video display\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "## Import the CLIP artifacts \n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
    "## Initiating tokenizer and encoder.\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "## Initiating the VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "## Initializing a scheduler and Setting number of sampling steps\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "scheduler.set_timesteps(50)\n",
    "## Initializing the U-Net model\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "## Helper functions\n",
    "def load_image(p):\n",
    "    '''\n",
    "    Function to load images from a defined path\n",
    "    '''\n",
    "    return Image.open(p).convert('RGB').resize((512,512))\n",
    "def pil_to_latents(image):\n",
    "    '''\n",
    "    Function to convert image to latents\n",
    "    '''\n",
    "    init_image = tfms.ToTensor()(image).unsqueeze(0) * 2.0 - 1.0\n",
    "    init_image = init_image.to(device=\"cuda\", dtype=torch.float16) \n",
    "    init_latent_dist = vae.encode(init_image).latent_dist.sample() * 0.18215\n",
    "    return init_latent_dist\n",
    "def latents_to_pil(latents):\n",
    "    '''\n",
    "    Function to convert latents to images\n",
    "    '''\n",
    "    latents = (1 / 0.18215) * latents\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    return pil_images\n",
    "def text_enc(prompts, maxlen=None):\n",
    "    '''\n",
    "    A function to take a texual promt and convert it into embeddings\n",
    "    '''\n",
    "    if maxlen is None: maxlen = tokenizer.model_max_length\n",
    "    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\") \n",
    "    return text_encoder(inp.input_ids.to(\"cuda\"))[0].half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_2_img(prompts, g=7.5, seed=100, steps=70, dim=512, save_int=False):\n",
    "    \"\"\"\n",
    "    Diffusion process to convert prompt to image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining batch size\n",
    "    bs = len(prompts) \n",
    "    \n",
    "    # Converting textual prompts to embedding\n",
    "    text = text_enc(prompts) \n",
    "    \n",
    "    # Adding an unconditional prompt , helps in the generation process\n",
    "    uncond =  text_enc([\"\"] * bs, text.shape[1])\n",
    "    emb = torch.cat([uncond, text])\n",
    "    \n",
    "    # Setting the seed\n",
    "    if seed: torch.manual_seed(seed)\n",
    "    \n",
    "    # Initiating random noise\n",
    "    latents = torch.randn((bs, unet.in_channels, dim//8, dim//8))\n",
    "    \n",
    "    # Setting number of steps in scheduler\n",
    "    scheduler.set_timesteps(steps)\n",
    "    \n",
    "    # Adding noise to the latents \n",
    "    latents = latents.to(\"cuda\").half() * scheduler.init_noise_sigma\n",
    "    \n",
    "    # Iterating through defined steps\n",
    "    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n",
    "        # We need to scale the i/p latents to match the variance\n",
    "        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "        \n",
    "        # Predicting noise residual using U-Net\n",
    "        with torch.no_grad(): u,t = unet(inp, ts, encoder_hidden_states=emb).sample.chunk(2)\n",
    "            \n",
    "        # Performing Guidance\n",
    "        pred = u + g*(t-u)\n",
    "        \n",
    "        # Conditioning  the latents\n",
    "        latents = scheduler.step(pred, ts, latents).prev_sample\n",
    "        \n",
    "        # Saving intermediate images\n",
    "        if save_int: \n",
    "            if not os.path.exists(f'./steps'):\n",
    "                os.mkdir(f'./steps')\n",
    "            latents_to_pil(latents)[0].save(f'steps/{i:04}.jpeg')\n",
    "            \n",
    "    # Returning the latent representation to output an image of 3x512x512\n",
    "    return latents_to_pil(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = prompt_2_img([\"A dog wearing a hat\", \"a photograph of an astronaut riding a horse\"], save_int=False)\n",
    "for img in images:display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "[How To Stable Diffusion](https://towardsdatascience.com/stable-diffusion-using-hugging-face-501d8dbdd8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
