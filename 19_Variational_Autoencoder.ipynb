{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "# Tutorial 19: Variational Autoencoder \n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Creating images with a variational autoencoder\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "- Python, PyTorch, Deep Learning Training, Stochastic Gradient Descent\n",
    "\n",
    "My contact:\n",
    "\n",
    "- Niklas Beuter (niklas.beuter@th-luebeck.de)\n",
    "\n",
    "Course:\n",
    "\n",
    "- Slides and notebooks will be available at https://lernraum.th-luebeck.de/course/view.php?id=5383\n",
    "\n",
    "## Expected Outcomes\n",
    "* Understand the feature generation process of the encoder\n",
    "* Understand the use of the decoder and why it is not necessary during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Variational Autoencoders (VAE)\n",
    "\n",
    "Variational Autoencoders (VAEs) are a class of generative models that are widely used for tasks such as image generation, data compression, and anomaly detection. Introduced by Kingma and Welling in 2013, VAEs combine techniques from deep learning and Bayesian inference to model complex data distributions.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Autoencoders\n",
    "An autoencoder is a type of neural network designed to learn a compressed representation of data. It consists of two main parts:\n",
    "- **Encoder**: This part of the network compresses the input data into a lower-dimensional latent representation.\n",
    "- **Decoder**: This part reconstructs the input data from the latent representation.\n",
    "\n",
    "### Variational Inference\n",
    "Variational inference is a technique in Bayesian statistics that approximates probability densities through optimization. Instead of computing exact posterior distributions, which can be computationally expensive, variational inference approximates these distributions with a simpler, parameterized distribution.\n",
    "\n",
    "### Latent Space\n",
    "In a VAE, the latent space is a continuous, multidimensional space where each point represents a possible compressed representation of the input data. The VAE is trained to ensure that similar inputs are mapped to nearby points in the latent space.\n",
    "\n",
    "## How VAE Works\n",
    "\n",
    "### Encoder\n",
    "The encoder network takes an input $ x $ and maps it to a mean $ \\mu $ and a standard deviation $ \\sigma $ of a Gaussian distribution in the latent space. This can be expressed as:\n",
    "$$ z \\sim \\mathcal{N}(\\mu(x), \\sigma(x)) $$\n",
    "where $ z $ is the latent vector.\n",
    "\n",
    "### Reparameterization Trick\n",
    "To enable backpropagation through the sampling process, VAEs use the reparameterization trick. Instead of sampling $ z $ directly from $ \\mathcal{N}(\\mu, \\sigma) $, we sample $ \\epsilon $ from a standard normal distribution and compute $ z $ as:\n",
    "$$ z = \\mu + \\sigma \\cdot \\epsilon \\$$\n",
    "where $ \\epsilon \\sim \\mathcal{N}(0, 1) $.\n",
    "\n",
    "### Decoder\n",
    "The decoder network takes the latent vector $ z $ and reconstructs the input data $ \\hat{x} $. The goal is for $ \\hat{x} $ to be as close as possible to the original input $ x $.\n",
    "\n",
    "### Loss Function\n",
    "The VAE loss function consists of two parts:\n",
    "- **Reconstruction Loss**: Measures how well the decoder reconstructs the input data.\n",
    "- **KL Divergence**: Regularizes the latent space by ensuring that the learned distribution $ q(z|x) $ is close to the prior $ p(z) $, typically a standard normal distribution.\n",
    "\n",
    "The combined loss is given by:\n",
    "$$ \\mathcal{L} = \\text{Reconstruction Loss} + \\text{KL Divergence} $$\n",
    "\n",
    "## Applications of VAE\n",
    "- **Image Generation**: VAEs can generate new images by sampling from the latent space and decoding.\n",
    "- **Data Compression**: VAEs can learn efficient representations of data, useful for compression.\n",
    "- **Anomaly Detection**: VAEs can identify anomalies by reconstructing data and measuring reconstruction error.\n",
    "\n",
    "## Conclusion\n",
    "Variational Autoencoders are powerful tools for learning compact, meaningful representations of data. By combining deep learning with probabilistic inference, VAEs enable various applications in generative modeling and beyond.\n",
    "\n",
    "For more detailed implementations and advanced topics, exploring the original paper by Kingma and Welling (2013) and subsequent literature is recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "The following code loads data from [MNIST](https://github.com/cvdfoundation/mnist), which are a lot of handdrawings from the digits 0-9.\n",
    "\n",
    "![MNIST Image](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
    "\n",
    "Idea is to learn an encoder, which is able to compress the input data to a low dimensional space (the latent space). As example we just use 20 latents, which are able to reconstruct the data again. \n",
    "\n",
    "At the end, we demonstrate that the latent space is continous, which allows to create a random vector and create a number out of it. We can even interpolate between numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision ipywidgets torchviz gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "## Setup Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# Hyperparameter\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 1\n",
    "latent_dim = 20\n",
    "image_size = 784  # 28*28 f√ºr MNIST\n",
    "\n",
    "# Make sure a result directory exists\n",
    "output_dir = os.path.join(os.getcwd(), 'results')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Prepare data, here we just load MNIST data as it is (digits from 0-9)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(-1))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# VAE-Modell\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size, h_dim, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        # following layer fc2 and fc3 are trained in parallel to output mu and logvar as variables for a probability distribution\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)  # mu layer\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)  # logvar layer (the log variance is used instead of the standard deviation directly due to numerical reasons)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "\n",
    "    # This is needed to be able to caluclate the Backpropagation as we do not have a single value, but a probability distribution as output\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar) # Trick to calculate the standard deviation out of logvar\n",
    "        eps = torch.randn_like(std) # returns values with the same size like std, but chosen from a standard normal distribution (mean=0, std=1)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc4(z))\n",
    "        return torch.sigmoid(self.fc5(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x) # return both, mean and the logvar \n",
    "        z = self.reparameterize(mu, logvar) # calculate the standard deviation\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Verlustfunktion\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Modell, Optimizer\n",
    "vae = VAE(image_size=image_size, h_dim=400, z_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "def saveImgs(model, latent_dim, epoch, output_dir):\n",
    "    with torch.no_grad():\n",
    "            sample = torch.randn(64, latent_dim).to(device)\n",
    "            results = model.decode(sample).cpu()\n",
    "\n",
    "            # Erstelle eine Collage aus Zufallsvektoren und generierten Bildern\n",
    "            for i in range(64):\n",
    "                fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "                ax[0].imshow(sample[i].cpu().numpy().reshape(1, -1), cmap='viridis', aspect='auto')\n",
    "                ax[0].set_title('Random Vector')\n",
    "                ax[0].axis('off')\n",
    "                \n",
    "                ax[1].imshow(results[i].view(28, 28), cmap='gray')\n",
    "                ax[1].set_title('Generated Image')\n",
    "                ax[1].axis('off')\n",
    "                            \n",
    "                plt.savefig(os.path.join(output_dir, f'sample_{epoch}_{i}.png'))\n",
    "                plt.close(fig)\n",
    "            #save_image(sample.view(64, 1, 28, 28), os.path.join(output_dir, f'sample_{epoch}.png'))\n",
    "\n",
    "# Generate new images\n",
    "def generate_images(model, num_images=64):\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(num_images, latent_dim).to(device)\n",
    "        results = model.decode(sample).cpu()\n",
    "\n",
    "        for i in range(num_images):\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "            ax[0].imshow(results[i].cpu().numpy().reshape(1, -1), cmap='viridis', aspect='auto')\n",
    "            ax[0].set_title('Random Vector')\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            ax[1].imshow(results[i].view(28, 28), cmap='gray')\n",
    "            ax[1].set_title('Generated Image')\n",
    "            ax[1].axis('off')\n",
    "\n",
    "            plt.savefig(os.path.join(output_dir, f'generated_images_{i}.png'))\n",
    "            plt.close(fig)\n",
    "\n",
    "        #save_image(sample.view(num_images, 1, 28, 28), os.path.join(output_dir, 'generated_images.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save latent representations (for interpolation later)\n",
    "latent_representations = {i: [] for i in range(10)}\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save latent representations (for interpolation later)\n",
    "        z = vae.reparameterize(mu, logvar)\n",
    "        for i, label in enumerate(labels):\n",
    "            latent_representations[label.item()].append(z[i].detach().cpu().numpy())\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "    # Save example images\n",
    "    #saveImgs(vae, latent_dim, epoch, output_dir)\n",
    "\n",
    "\n",
    "#generate_images(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Interpolation zwischen zwei latenten Vektoren\n",
    "def interpolate_vectors(v1, v2, num_steps=10):\n",
    "    vectors = []\n",
    "    for alpha in np.linspace(0, 1, num_steps):\n",
    "        interpolated = (1 - alpha) * v1 + alpha * v2\n",
    "        vectors.append(interpolated)\n",
    "    return torch.stack(vectors)\n",
    "\n",
    "# Generierung von interpolierten Bildern\n",
    "def generate_interpolated_images_between_digits(model, digit1, digit2, num_interpolations=10):\n",
    "    with torch.no_grad():\n",
    "        z1 = torch.tensor(latent_representations[digit1][-1]).to(device)  # W√§hle einen latenten Vektor f√ºr die erste Zahl\n",
    "        z2 = torch.tensor(latent_representations[digit2][-1]).to(device)  # W√§hle einen latenten Vektor f√ºr die zweite Zahl\n",
    "        interpolated_vectors = interpolate_vectors(z1, z2, num_interpolations).to(device)\n",
    "        recon_images = model.decode(interpolated_vectors).cpu()\n",
    "\n",
    "        if not os.path.exists('./results'):\n",
    "            os.makedirs('./results')\n",
    "\n",
    "        for i in range(num_interpolations):\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "            random_vector = interpolated_vectors[i].cpu().numpy().reshape(1, -1)\n",
    "            ax[0].imshow(random_vector, cmap='viridis', aspect='auto')\n",
    "            ax[0].set_title('Interpolated Vector')\n",
    "            ax[0].axis('off')\n",
    "            \n",
    "            ax[1].imshow(recon_images[i].view(28, 28), cmap='gray')\n",
    "            ax[1].set_title('Generated Image')\n",
    "            ax[1].axis('off')\n",
    "            \n",
    "            plt.savefig(f'./results/interpolated_image_{digit1}_to_{digit2}_{i}.png')\n",
    "            plt.close(fig)\n",
    "\n",
    "generate_interpolated_images_between_digits(vae, digit1=4, digit2=7, num_interpolations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Interpolation zwischen zwei latenten Vektoren\n",
    "def interpolate_vectors2(v1, v2, alpha):\n",
    "    return (1 - alpha) * v1 + alpha * v2\n",
    "\n",
    "# Initialisiere zwei zuf√§llige latente Vektoren\n",
    "z1 = torch.randn(latent_dim).to(device)\n",
    "z2 = torch.randn(latent_dim).to(device)\n",
    "\n",
    "# Erstelle die Interpolationsfunktion\n",
    "def plot_interpolation(alpha):\n",
    "    z_interpolated = interpolate_vectors2(z1, z2, alpha)\n",
    "    recon_image = vae.decode(z_interpolated).cpu().detach().numpy().reshape(28, 28)\n",
    "    \n",
    "    plt.imshow(recon_image, cmap='gray')\n",
    "    plt.title(f'Interpolation: alpha={alpha:.2f}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Erstelle den Slider\n",
    "alpha_slider = widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.5, description='Alpha:')\n",
    "a_s = widgets.FloatSlider(min=-1.0, max=1.0, step=0.01, value=0.5, description='a:')\n",
    "\n",
    "widgets.interact(plot_interpolation, alpha=alpha_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle die Interpolationsfunktion f√ºr den latenten Vektor\n",
    "def plot_latent_vector(**latent_values):\n",
    "    z = torch.tensor([latent_values[f'z{i}'] for i in range(latent_dim)], dtype=torch.float32).to(device)\n",
    "    recon_image = vae.decode(z).cpu().detach().numpy().reshape(28, 28)\n",
    "\n",
    "    plt.imshow(recon_image, cmap='gray')\n",
    "    plt.title(f'Latent Vector: {latent_values}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Erstelle die Slider f√ºr den latenten Vektor\n",
    "sliders = [widgets.FloatSlider(min=-3.0, max=3.0, step=0.1, value=0.0, description=f'z{i}') for i in range(latent_dim)]\n",
    "\n",
    "# Erstelle die Interaktivit√§t\n",
    "ui = widgets.VBox(sliders)\n",
    "out = widgets.interactive_output(plot_latent_vector, {f'z{i}': sliders[i] for i in range(latent_dim)})\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load the model to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': vae.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': train_loss,\n",
    "    }, f'vae_checkpoint_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell laden\n",
    "checkpoint = torch.load('vae_checkpoint_epoch_99.pth')\n",
    "vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "train_loss = checkpoint['loss']\n",
    "\n",
    "#vae.eval()  # Setze das Modell in den Evaluierungsmodus, falls erforderlich\n",
    "# oder\n",
    "vae.train()  # Setze das Modell in den Trainingsmodus, falls erforderlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate faces\n",
    "\n",
    "We are using the celebA dataset. Downloading it is not always possible as it is hosted on gdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "# Hyperparameter\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 1\n",
    "latent_dim = 20\n",
    "image_size = 64  # 64x64 f√ºr CelebA-Bilder\n",
    "\n",
    "# Datenvorbereitung\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0\n",
    "\n",
    "# Verzeichnis, in das die Bilder entpackt wurden\n",
    "data_dir = './data/celeba/img_align_celeba'  # Ersetze 'path_to_extract_directory' durch den tats√§chlichen Pfad\n",
    "\n",
    "train_dataset = CelebADataset(root_dir=data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# VAE-Modell\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=256, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(h_dim * 4 * 4, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim * 4 * 4, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return self.fc1(h), self.fc2(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # There is also a function in pytorch for that\n",
    "        # std = torch.exp(0.5 * logvar)\n",
    "        # return torch.distributions.Normal(loc=mu, scale=std).rsample()  # rsample allows pathwise derivatives, i.e. implements the reparametrization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std) # returns values with the same size like std, but chosen from a standard normal distribution (mean=0, std=1)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc3(z)\n",
    "        h = h.view(h.size(0), 256, 4, 4)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Verlustfunktion\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# Modell, Optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vae = VAE(image_channels=3, h_dim=256, z_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Funktion zum Anzeigen von Trainingsbildern\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Anzeige der Trainingsbilder vor dem Training\n",
    "real_batch = next(iter(train_loader))\n",
    "images, labels = real_batch  # Entpacke die Bilder und Labels\n",
    "plt.figure(figsize=(8, 16))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "imshow(vutils.make_grid(images[:32], padding=2, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        # Stelle sicher, dass die Zielbilder im Bereich [0, 1] liegen\n",
    "        loss = loss_function(recon_batch, (data + 1) / 2, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss / len(train_loader.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaktive Slider zur Steuerung der latenten Vektoren\n",
    "def plot_latent_vector(**latent_values):\n",
    "    z = torch.tensor([latent_values[f'z{i}'] for i in range(latent_dim)], dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        recon_image = vae.decode(z.unsqueeze(0)).cpu().detach().numpy().transpose(0, 2, 3, 1).squeeze()\n",
    "        recon_image = (recon_image + 1) / 2  # Reskalieren auf [0, 1] f√ºr die Anzeige\n",
    "\n",
    "    plt.imshow(recon_image)\n",
    "    plt.title(f'Latent Vector: {latent_values}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Erstelle die Slider f√ºr den latenten Vektor\n",
    "sliders = [widgets.FloatSlider(min=-3.0, max=3.0, step=0.1, value=0.0, description=f'z{i}') for i in range(latent_dim)]\n",
    "\n",
    "# Erstelle die Interaktivit√§t\n",
    "ui = widgets.VBox(sliders)\n",
    "out = widgets.interactive_output(plot_latent_vector, {f'z{i}': sliders[i] for i in range(latent_dim)})\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': vae.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': train_loss,\n",
    "    }, f'vae_celeb_checkpoint_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modell laden\n",
    "checkpoint = torch.load('vae_celeb_checkpoint_epoch_9.pth')\n",
    "vae.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "# train_loss = checkpoint['loss']\n",
    "\n",
    "#vae.eval()  # Setze das Modell in den Evaluierungsmodus, falls erforderlich\n",
    "# oder\n",
    "vae.train()  # Setze das Modell in den Trainingsmodus, falls erforderlich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "This notebook is adapted from or uses following sources:\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
